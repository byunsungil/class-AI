{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/byunsungil/AI01/blob/main/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5_%ED%81%AC%EB%A1%A4%EB%A7%81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f662f6eb",
      "metadata": {
        "id": "f662f6eb"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "import time\n",
        "from openpyxl import Workbook\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f96b24a",
      "metadata": {
        "id": "5f96b24a"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
        "driver.maximize_window()\n",
        "a=input(\"유튜브명 : \")\n",
        "url = \"https://www.youtube.com/results?search_query={}\".format(a)\n",
        "driver.get(url)\n",
        "driver.implicitly_wait(3)\n",
        "time.sleep(1.5) \n",
        "element = driver.find_element(By.ID, \"text\") \n",
        "element.click()\n",
        "element = driver.find_element(By.LINK_TEXT, \"동영상\")\n",
        "element.click()\n",
        "time.sleep(3) \n",
        "last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "num_scrolls = 100  \n",
        "url_list = [] \n",
        "\n",
        "for i in range(num_scrolls): \n",
        "    #처음부터 끝까지 스크롤을 내려 해당내용 크롤링\n",
        "    driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight)\")\n",
        "    time.sleep(1.5)\n",
        "    # 스크롤을 내렸을 때 더 이상 내용이 없을 때\n",
        "    new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    \n",
        "    if new_height == last_height: \n",
        "        break\n",
        "    \n",
        "    html_source = driver.page_source  \n",
        "    soup = BeautifulSoup(html_source, \"html.parser\")\n",
        "\n",
        "# 동영상의 url이 포함된 요소를 찾기 \n",
        "parent_elements = soup.find_all(class_ = 'yt-simple-endpoint style-scope ytd-playlist-thumbnail')\n",
        "# 동영상의 요소의 href값을 추출해 저장하기\n",
        "for parent_element in parent_elements:\n",
        "    url = parent_element['href']\n",
        "    url_list.append('https://www.youtube.com' + url)\n",
        "\n",
        "# 저장된 url 리스트를 프린트 하시오\n",
        "url_list=list(set(url_list))\n",
        "print(url_list)\n",
        "\n",
        "#유료 가입 광고 무시하기\n",
        "try:\n",
        "    driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47a08283",
      "metadata": {
        "id": "47a08283"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(\"chromedriver.exe\")\n",
        "\n",
        "urls=url_list[0:2]\n",
        "id =[]\n",
        "comment=[]\n",
        "\n",
        "for url in urls:\n",
        "    driver.get(url)\n",
        "    driver.implicitly_wait(3)\n",
        "\n",
        "    time.sleep(3)\n",
        "\n",
        "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
        "        time.sleep(1.5)\n",
        "\n",
        "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
        "        if new_height == last_height:\n",
        "            break\n",
        "        last_height = new_height\n",
        "\n",
        "    time.sleep(1.5)\n",
        "\n",
        "    try:\n",
        "        driver.find_element_by_css_selector(\"#dismiss-button > a\").click()\n",
        "    except:\n",
        "            pass\n",
        "        \n",
        "    html_source = driver.page_source\n",
        "    soup = BeautifulSoup(html_source, 'html.parser')\n",
        "\n",
        "    id_list = soup.select(\"div#header-author > h3 > #author-text > span\")\n",
        "    comment_list = soup.select(\"yt-formatted-string#content-text\")\n",
        "    for i, j in zip(id_list,comment_list):\n",
        "        id.append(i)\n",
        "        comment.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e384f816",
      "metadata": {
        "id": "e384f816"
      },
      "outputs": [],
      "source": [
        "id_final = []\n",
        "comment_final = []\n",
        "\n",
        "for i in range(len(id)):\n",
        "    temp_id = id[i].text\n",
        "    temp_id = temp_id.replace('\\n', '')\n",
        "    temp_id = temp_id.replace('\\t', '')\n",
        "    temp_id = temp_id.replace('    ', '')\n",
        "    id_final.append(temp_id) # 댓글 작성자\n",
        "\n",
        "    temp_comment = comment[i].text\n",
        "    temp_comment = temp_comment.replace('\\n', '')\n",
        "    temp_comment = temp_comment.replace('\\t', '')\n",
        "    temp_comment = temp_comment.replace('    ', '')\n",
        "    comment_final.append(temp_comment) # 댓글 내용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c880f28e",
      "metadata": {
        "id": "c880f28e"
      },
      "outputs": [],
      "source": [
        "# 저장된 내용을 아이디, 댓글 내용를 가지도록 dict으로 저장\n",
        "pd_data = {\"아이디\" : id_final, \"댓글 내용\" : comment_final}\n",
        "\n",
        "# 데이터 프레임으로 변환\n",
        "youtube_pd = pd.DataFrame(pd_data)\n",
        "\n",
        "# 데이터를 xlsx로 저장 index=False 데이터의 순서 번호 제거\n",
        "youtube_pd.to_excel(\"yo.xlsx\", index=False)\n",
        "df = pd.read_excel(\"yo.xlsx\")\n",
        "df.to_csv(\"yo.csv \", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073b0621",
      "metadata": {
        "id": "073b0621"
      },
      "outputs": [],
      "source": [
        "df[\"아이디\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3ab9403",
      "metadata": {
        "id": "b3ab9403"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "\n",
        "for f in fm.fontManager.ttflist:\n",
        "    if 'Nanum' in f.name:\n",
        "        print(f.name, f.fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b656ea",
      "metadata": {
        "id": "d8b656ea"
      },
      "outputs": [],
      "source": [
        "# 폰트 경로 설정\n",
        "font_path = 'C:/Users/user/AppData/Local/Microsoft/Windows/Fonts/NanumSquareL.ttf'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95939575",
      "metadata": {
        "id": "95939575"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"ㅋㅋ\")\n",
        "stopwords.add(\"ㅎㅎ\")\n",
        "stopwords.add(\"너무\")\n",
        "stopwords.add(\"진짜\")\n",
        "stopwords.add(\"와\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c4507e6",
      "metadata": {
        "id": "2c4507e6"
      },
      "outputs": [],
      "source": [
        "stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bca2a42",
      "metadata": {
        "id": "5bca2a42"
      },
      "outputs": [],
      "source": [
        "with open(\"stopwords.txt\", encoding=\"utf-8\") as f:\n",
        "    stopwords = set(f.read().splitlines())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb27e918",
      "metadata": {
        "id": "eb27e918"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# CSV 파일을 pandas 데이터프레임으로 읽어옴\n",
        "df = pd.read_csv('파일이름.csv',encoding='utf-8')\n",
        "\n",
        "# 댓글 내용이 저장된 컬럼 선택\n",
        "comment_col = '댓글 내용'\n",
        "comment_list = df[comment_col].tolist()\n",
        "new_list = []\n",
        "for x in comment_list:\n",
        "    new_list.append(str(x))\n",
        "\n",
        "# 모든 댓글 내용을 하나의 문자열로 결합\n",
        "text = ' '.join(new_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c43e549",
      "metadata": {
        "id": "9c43e549"
      },
      "outputs": [],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb6658d",
      "metadata": {
        "id": "afb6658d"
      },
      "outputs": [],
      "source": [
        "# 워드 클라우드 생성\n",
        "wordcloud = WordCloud(font_path=font_path,width=800, height=800,\n",
        "                      background_color='white',stopwords=stopwords, min_word_length=4).generate(text)\n",
        "\n",
        "# 단어의 빈도 계산\n",
        "wordcloud.generate_from_frequencies(wordcloud.process_text(text.lower()))\n",
        "word_freq = wordcloud.process_text(text.lower())\n",
        "\n",
        "# 워드 클라우드 출력\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad=0)\n",
        "\n",
        "# 파일로 저장\n",
        "plt.savefig('wordcloud.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c12e372",
      "metadata": {
        "id": "1c12e372"
      },
      "outputs": [],
      "source": [
        "##문제 정답\n",
        "\n",
        "# 해당 숫자보다 낮은 빈도의 단어를 도출함\n",
        "word_num = {}\n",
        "for word, freq in word_freq.items():\n",
        "    if freq < 3: # 해당 단어의 빈도수 보다 낮은 단어 도출\n",
        "        word_num[word] = freq\n",
        "\n",
        "# 낮은 빈도의 단어를 프린트함\n",
        "for word, frequency in word_num.items():\n",
        "    print((\"{}: {}\").format(word, frequency))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e859e13",
      "metadata": {
        "id": "2e859e13"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ad2abf",
      "metadata": {
        "id": "34ad2abf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca38545b",
      "metadata": {
        "id": "ca38545b"
      },
      "outputs": [],
      "source": [
        "def train_sentiment_analysis_model(positive_file, negative_file):\n",
        "    # 긍정 단어와 부정 단어를 읽어서 리스트로 변환합니다.\n",
        "    with open(positive_file, 'r', encoding='utf-8') as f:\n",
        "        positive_words = f.read().splitlines()\n",
        "\n",
        "    with open(negative_file, 'r', encoding='utf-8') as f:\n",
        "        negative_words = f.read().splitlines()\n",
        "\n",
        "    # 긍정적인 문장과 부정적인 문장을 학습 데이터로 생성합니다.\n",
        "    positive_sentences = [\" \".join(positive_words)] * len(positive_words)\n",
        "    negative_sentences = [\" \".join(negative_words)] * len(negative_words)\n",
        "\n",
        "    # 학습 데이터와 레이블을 생성합니다.\n",
        "    X = positive_sentences + negative_sentences\n",
        "    y = [1] * len(positive_sentences) + [0] * len(negative_sentences)\n",
        "\n",
        "    # CountVectorizer를 사용하여 단어의 빈도수를 측정합니다.\n",
        "    vectorizer = CountVectorizer(token_pattern=r\"\\b\\w+\\b\") \n",
        "    X = vectorizer.fit_transform(X)\n",
        "\n",
        "    # LogisticRegression을 사용하여 모델을 학습합니다.\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    return vectorizer, clf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "pos_list = []\n",
        "neg_list = []\n",
        "positive_list=[]\n",
        "negative_list=[]\n",
        "pos_convert=[]\n",
        "neg_convert=[]\n",
        "\n",
        "def train_sentiment_analysis_model(positive_file, negative_file):\n",
        "    # 긍정 단어와 부정 단어를 읽어서 리스트로 변환합니다.\n",
        "    with open(positive_file, 'r', encoding='utf-8') as f:\n",
        "        positive_words = f.read().splitlines()\n",
        "\n",
        "    with open(negative_file, 'r', encoding='utf-8') as f:\n",
        "        negative_words = f.read().splitlines()\n",
        "\n",
        "    max_permutations = 20\n",
        "\n",
        "    pos_per = list(itertools.islice(itertools.permutations(positive_words), max_permutations))\n",
        "    neg_per = list(itertools.islice(itertools.permutations(negative_words), max_permutations))\n",
        "\n",
        "    for p in positive_sentences:\n",
        "        pos_list.append(list(p))\n",
        "\n",
        "    for i in range(0,len(pos_list)):\n",
        "        a=[\" \".join(pos_list[i])]\n",
        "        positive_list.append(a)\n",
        "\n",
        "    for num in positive_list:\n",
        "        pos_convert.append(num[0])\n",
        "    \n",
        "    for n in negative_sentences:\n",
        "        neg_list.append(list(n))\n",
        "\n",
        "    for i in range(0,len(neg_list)):\n",
        "        a=[\" \".join(neg_list[i])]\n",
        "        negative_list.append(a)\n",
        "\n",
        "    for num in negative_list:\n",
        "        neg_convert.append(num[0])\n",
        "\n",
        "    # 학습 데이터와 레이블을 생성합니다.\n",
        "    X = pos_convert + neg_convert\n",
        "    y = [1] * len(pos_convert) + [0] * len(neg_convert)\n",
        "\n",
        "    # CountVectorizer를 사용하여 단어의 빈도수를 측정합니다.\n",
        "    vectorizer = CountVectorizer(token_pattern=r\"\\b\\w+\\b\")\n",
        "    X = vectorizer.fit_transform(X)\n",
        "    # LogisticRegression을 사용하여 모델을 학습합니다.\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(X, y)\n",
        "\n",
        "    return vectorizer, clf"
      ],
      "metadata": {
        "id": "Dg78AgE-0cd3"
      },
      "id": "Dg78AgE-0cd3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a1e92c",
      "metadata": {
        "id": "e4a1e92c"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment_analysis(text, vectorizer, clf):\n",
        "    # 입력된 텍스트를 벡터화합니다.\n",
        "    X = vectorizer.transform([text])\n",
        "\n",
        "    # 모델을 사용하여 감성을 예측합니다.\n",
        "    y_pred = clf.predict(X)\n",
        "\n",
        "    # 예측 결과에 따라 출력 메시지와 카운트 정보를 선택합니다.\n",
        "    if y_pred[0] == 1:\n",
        "        result = {\"sentiment\": \"긍정적인 단어\", \"positive_count\": 1, \"negative_count\": 0}\n",
        "    else:\n",
        "        result = {\"sentiment\": \"부정적인 단어\", \"positive_count\": 0, \"negative_count\": 1}\n",
        "\n",
        "    # 입력된 텍스트에 포함된 긍정 단어와 부정 단어를 카운트합니다.\n",
        "    for word in text.split():\n",
        "        if word in positive_words:\n",
        "            result[\"positive_count\"] += 1\n",
        "        elif word in negative_words:\n",
        "            result[\"negative_count\"] += 1\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7a4127",
      "metadata": {
        "id": "5b7a4127"
      },
      "outputs": [],
      "source": [
        "with open('text/positive_words.txt', 'r',encoding='utf-8') as f:\n",
        "    positive_words = f.read().splitlines()\n",
        "\n",
        "with open('text/negative_words.txt', 'r',encoding='utf-8') as f:\n",
        "    negative_words = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0086eba4",
      "metadata": {
        "id": "0086eba4"
      },
      "outputs": [],
      "source": [
        "vectorizer, clf=train_sentiment_analysis_model(\"text/positive_words.txt\",\"text/negative_words.txt\")\n",
        "text = input(\"분석할 문자열을 입력하세요: \")\n",
        "\n",
        "predict_sentiment_analysis(text, vectorizer, clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e43f923",
      "metadata": {
        "id": "1e43f923"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"yo.csv\", encoding='utf-8-sig')\n",
        "comment_col = '댓글 내용'\n",
        "comment_list = df[comment_col].tolist()\n",
        "new_list = []\n",
        "for x in comment_list:\n",
        "    new_list.append(str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f20cf85",
      "metadata": {
        "id": "8f20cf85"
      },
      "outputs": [],
      "source": [
        "##문제 정답\n",
        "emotion = []\n",
        "for comment in new_list:\n",
        "    prediction = predict_sentiment_analysis(comment, vectorizer, clf)\n",
        "    emotion.append(prediction['sentiment'])\n",
        "\n",
        "df[\"감정\"]=pd.DataFrame(emotion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f60332",
      "metadata": {
        "id": "47f60332"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6502b97",
      "metadata": {
        "id": "e6502b97"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "text",
      "language": "python",
      "name": "text"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}